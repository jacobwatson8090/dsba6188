{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sa7FrJ7-903S",
        "outputId": "575eff24-1a3b-48b9-bf45-14363c00f322"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n",
            "Topic 1:\n",
            "general caseys us dollar delek foods air huntington products national\n",
            "Topic 2:\n",
            "american axle republic huntington td old ingalls data home fidelity\n",
            "Topic 3:\n",
            "united states us foods delek dollar air products alaska westinghouse\n",
            "Topic 4:\n",
            "financial southern citizens securian western pnc services for thrivent us\n",
            "Topic 5:\n",
            "of bank packaging corp life group reinsurance intl expeditors labratory\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import NMF\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "import nltk\n",
        "import gensim.downloader as api\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Load the JSON file and extract text data\n",
        "with open('consolidated_data.json', 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# Define financial terms for filtering\n",
        "seed_words = ['financial', 'bank', 'economy', 'market', 'investment', 'revenue', 'profit', 'capital', 'asset', 'income', 'equity', 'debt', 'loan', 'stock', 'bond', 'dividend']\n",
        "\n",
        "# Download NLTK stopwords data\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load pre-trained Word2Vec model\n",
        "word2vec_model = api.load('word2vec-google-news-300')\n",
        "\n",
        "# Train or fine-tune word embeddings specifically on financial text data\n",
        "financial_sentences = [word_tokenize(text.lower()) for text in data]\n",
        "financial_word2vec_model = Word2Vec(financial_sentences, vector_size=300, window=5, min_count=1)\n",
        "\n",
        "# Define a function to filter text data based on financial terms\n",
        "def filter_financial_text(text, threshold):\n",
        "    tokens = word_tokenize(text.lower())  # Tokenize text and convert to lowercase\n",
        "    # Calculate average cosine similarity between tokens and seed words using domain-specific word embeddings\n",
        "    avg_cosine_similarity = sum(financial_word2vec_model.wv.similarity(token, seed_word) for token in tokens for seed_word in seed_words) / len(tokens)\n",
        "    return avg_cosine_similarity > threshold  # Filter based on a threshold\n",
        "\n",
        "# Define a function to preprocess the text data\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()  # Convert text to lowercase\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
        "    tokens = [token for token in word_tokenize(text) if 'txt' not in token]  # Exclude tokens containing 'txt'\n",
        "    return ' '.join(tokens)  # Join tokens into a single string\n",
        "\n",
        "# Preprocess the text data\n",
        "preprocessed_texts = [preprocess_text(text) for text in data]\n",
        "\n",
        "# Vectorize the preprocessed text data using TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(preprocessed_texts)\n",
        "\n",
        "# Apply Non-Negative Matrix Factorization (NMF) using TF-IDF vectorized data\n",
        "num_topics = 5  # Specify the number of topics\n",
        "nmf_model = NMF(n_components=num_topics, random_state=42)\n",
        "nmf_model.fit(X_tfidf)\n",
        "\n",
        "# Display the topics generated by NMF\n",
        "def display_topics(model, feature_names, num_top_words):\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        print(f\"Topic {topic_idx + 1}:\")\n",
        "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-num_top_words - 1:-1]]))\n",
        "\n",
        "# Display the topics\n",
        "num_top_words = 10  # Specify the number of top words to display for each topic\n",
        "feature_names_tfidf = tfidf_vectorizer.get_feature_names_out()\n",
        "display_topics(nmf_model, feature_names_tfidf, num_top_words)\n"
      ]
    }
  ]
}